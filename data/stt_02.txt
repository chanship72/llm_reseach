[00:00.000 --> 00:04.800]  한 번쯤은 들어보셨을 거에요. 선형 획이라고 하죠.

[00:04.800 --> 00:12.320]  일단 실제 데이터가 이렇게 빨간색처럼 나타나 있으면

[00:12.320 --> 00:19.600]  그거를 fit하는 1차 함수의 y는 fx를 맞춰주는게 리니어 리그레이션 입니다.

[00:19.600 --> 00:26.800]  지금 사실 y는 fx 플러스 에로텀을 가지는게

[00:26.800 --> 00:34.400]  저희가 이제 항상 머신러닝이나 딥러닝이나 ai 혹은 통계적인 방법으로도 찾아야 되는

[00:34.400 --> 00:42.760]  이제 옵젝티브인데 fx 자체를 추정하는 것은 머신러닝을 딥러닝으로

[00:42.760 --> 00:51.760]  보통 많이 하죠. 근데 이제 모델을 잘 만든다는 개념은 y는 fx 에서 fx 를 추정하는 것도 중요하지 만

[00:51.760 --> 01:00.960]  y는 fx 에서 fx 를 ax 플러스 b 혹은 ax제오 플러스 bx 플러스 c 로 정의한다고 했을 때 a와 b 혹은 abc 를

[01:00.960 --> 01:08.360]  추정하는 것도 모델을 잘 만든다고도 얘기합니다. 즉 이게 어떻게 설명하면 되냐면

[01:08.360 --> 01:17.800]  리니어 리그레이션을 수행하기 위해서도 y는 ax 플러스 b 라는 함수를 만들기 위해서도 a와 b 를 추정하는 머신러닝 기법이 들어갈 수도 있고

[01:17.800 --> 01:25.520]  아니면 a와 b 를 추정하는 통계적인 방법이 들어갈 수도 있는 거죠. 그러니까 데이터를 보고 이거를 리니어 리그레이션으로 해석할지

[01:25.520 --> 01:34.760]  아니면 그냥 바로 y는 fx 를 머신러닝이나 딥러닝으로 해석해서 함수를 만들지는 여러분들이 판단하고 수행해야 되고

[01:34.760 --> 01:38.760]  데이터의 양에 따라서 다르겠죠?

[01:38.760 --> 01:50.240]  아무튼 항상 그랬듯이 데이터를 보고 y는 fx 플러스 error term에 fx 를 먼저 추정하고 그리고 error term을 최소화하는

[01:50.240 --> 01:56.400]  옵젝티브를 가진 모델을 만드는게 저희의 목적입니다.

[01:56.400 --> 02:05.320]  그래서 과정을 해보면 y는 β0 플러스 β1 x 플러스 s 라고 하면은

[02:05.320 --> 02:14.800]  저희가 구해야 할 텀들이 β0, β1 그리고 s 는 당연히 아이어스 텀이니까

[02:14.800 --> 02:23.400]  아이어스라고 하면 이제 에러 텀이니까 그냥 무시한다고 하고 그 β0 와 β1 을 추정해야 되는데

[02:23.400 --> 02:30.920]  그 리니어 리그레이션에서 추정하는 옵젝티브 펑션은 rss 라고 합니다.

[02:30.920 --> 02:40.640]  residual sum of squares 라고 하는데 이거는 한번쯤은 보셨을 거에요. 그냥 실제 값이랑

[02:40.640 --> 02:52.080]  저희가 추정한 값의 차의 제곱이니까 그냥 차라고 생각하시면 됩니다. 이렇게 해서 이 rss 를 최소 화하는 방향으로 해야지 적절한 리니어 리그레이션이

[02:52.080 --> 02:58.800]  만들어지겠죠. 그래서 이제 아까 앞서 말했던 β1 이라는

[02:58.800 --> 03:09.920]  이 슬로프와 β0 라는 y절편을 지금 밑에 있는 이 수식으로 나타낼 수 있는데

[03:09.920 --> 03:16.240]  아 죄송합니다. 이 위에 있는 수식이

[03:16.240 --> 03:28.720]  x와 y의 공분산식이거든요. x와 y의 공분산식이고 밑이 x의 분산식입니다.

[03:28.720 --> 03:37.400]  이런식으로 x와 y의 공분산과 x의 분산을 통해서 β1과 β0 를 구하고 그리고 rss 를 미니마이즈 하는

[03:37.400 --> 03:48.040]  미니마이즈 하는 방법은 여러가지가 있겠죠. 미분을 한다거나 아니면 나중에 가르쳐 드릴 그래디엔 디 샘플을 해서 미니마이즈 하는 방법과 동등으로

[03:48.040 --> 03:59.440]  y는 ax 플러스 b, y는 β제곱 플러스 β1 x 플러스 애로텀 이런식으로 가장 적정한 리니어 리그레이션을 추정합니다.

[03:59.440 --> 04:05.600]  제가 다음 시간에는

[04:05.960 --> 04:16.920]  로지스트 리그레이션과 리니어 리그레이션의 차이를 설명드리고, 리니어 리그레이션이 왜 중요하고, 리니어 리그레이션이 사실은 데이터가 이렇게

[04:16.920 --> 04:25.600]  복잡하게 나와 있거나 아니면은 조금 어떠한 경향성을 띄는 데이터는 거의 다 리니어 리그레이션으 로 풀어도 어느정도 추정을 할 수 있기 때문에

[04:25.600 --> 04:37.520]  리니어 리그레이션의 가장 중요한 턴인 rss 를 꼭 기억해 두시고 rss 를 미니마이즈 하는식으로 학 습을 한다는거 기억해 두시면 좋을 것 같습니다.

[04:37.520 --> 04:45.960]  그래서 리니어 리그레이션에 해당하는 내용을 사이클런으로 한번 구현을 해 볼텐데요.

[04:45.960 --> 04:57.480]  이거는 제가 저번 시간에도 얘기했듯이 사이클런, sk런으로 불러지고 그리고 또 sk런 모듈에

[04:57.480 --> 05:06.440]  리니어 리그레이션, 로지스틱 리그레이션 등등 여러가지 리그레이션 트리가 많습니다. 그리고 또 제가 다음 시간에는 dc전트리도 설명드리고

[05:06.440 --> 05:15.320]  그리고 knn 클래시피케이션 그리고 클러스터링 등등의 방법도 이론 먼저 설명드리고 사이클런으로  한번 구현까지

[05:15.320 --> 05:18.840]  실습을 진행하도록 하겠습니다.

[05:18.840 --> 05:25.680]  리니어 리그레이션을 먼저 시작할텐데

[05:25.680 --> 05:32.160]  리그레이션의 다양한 형태가 있다고 제가 말씀드렸죠. 리니어 리그레이션도 있고 아니면

[05:32.160 --> 05:41.920]  저번 시간에 얘기했던 좀 variance가 크고 complex가 높아진 모델 즉 non-linear한 리그레이션

[05:41.920 --> 05:51.120]  이거 같은 경우에는 다차항의 리그레이션 모델이 되겠죠. 혹은 로지스틱 리그레이션이라고 하는 클 래시피케이션의 한 형태의

[05:51.120 --> 05:58.520]  로지스틱 리그레이션도 있고 다양한 형태가 있는데 여기에서 저는 지금 사이클런의 리니어 리그레이션 모델을 한 뒤에 실습도

[05:58.520 --> 06:07.760]  이거와 병행하면서 하도록 하겠습니다. 아까도 말씀드렸듯이 y는 beta0 플러스 beta1 x 플러스

[06:07.760 --> 06:14.160]  beta n x n 까지 있겠죠. 이런식으로 해서 beta0와 beta1을 추정하는 방법인 리니어 리그레이션을

[06:14.160 --> 06:24.080]  설명 드릴텐데 만약에 x 데이터가 0.5, 0.7, 1.2, 3.6, 3.84 이런식으로 나와있고 y가 2, 2.5, 3.4,

[06:24.080 --> 06:34.000]  3.4에서 9까지 나타나와 있을 때 이제 주어진 데이터로 y는 ax 플러스 b 혹은 y는 beta0 플러스 beta1 x 를 추정한 뒤

[06:34.000 --> 06:41.360]  새로운 3.1 이라는 데이터가 왔을 때 어떻게 y를 추정하는지 그거에 대한 컨셉이 리니어 리그레이션입니다.

[06:41.360 --> 06:49.440]  그러니까 일반적으로 데이터가 이런식으로 트레이닝 데이터가 있는 경우가 있을 때 만약에 복잡하게 머신러닝 모델이나

[06:49.440 --> 06:56.240]  집러닝 모델로 했으면 여기 y나 fx를 막 복잡하게 찾을 수도 있고 아니면은 다양함수로

[06:56.240 --> 07:04.080]  추정한다고 했을 때는 복잡하게 찾을 수도 있는데 테스트 데이터가 왔을 때는 만약에 복잡하게 찾은 함수 같은 경우에는 트레이닝 헤러에선 잘 맞겠지만

[07:04.080 --> 07:12.240]  그러니까 트레이닝 데이터에서는 잘 맞을 수 있겠지만 테스트 데이터에서는 잘 못 맞추는 경우도 많이 발생합니다.

[07:12.240 --> 07:21.840]  그래서 제가 아까 말씀드렸던 리니어 리그레이션의 경우에 보통 테스트 헤러도 트레이닝 헤러와 잘 따라가는 성능을 보이기 때문에

[07:21.840 --> 07:32.840]  리니어 리그레이션도 정말 잘 사용하면 복잡한 데이터도 잘 프레딧 할 수도 있고 테스트 헤러도 많 이 줄일 수 있습니다.

[07:38.120 --> 07:44.880]  한번 실습을 통해서 해볼텐데 제가 저번 시간에 얘기 드렸던 fit 함수와

[07:44.880 --> 07:53.840]  프레딧 함수를 잘 기억해 주시면 기본 실습을 하는데 많은 도움을 받을 수 있을 거에요.

[07:53.840 --> 08:04.720]  fit 함수는 제가 저번에 말씀드렸듯이 머신러닝이나 딥러닝이나 혹은 다른 데이터 모델 방법에 있어서 연산을 하는 과정을

[08:04.720 --> 08:13.880]  fit이라고 했고 복잡한 데이터일수록 fit 하는데 시간이 많이 걸리고 함수를 추정하는 시간이 많이 걸리게 됩니다.

[08:13.880 --> 08:27.920]  그래서 fit을 한 다음에 리니어 리그레이션 모델을 가지고 프레딧을 하면 아까 예제로 보여드렸던 x에 3.1을 넣어서 y 박스를 추정할 수 있는

[08:27.920 --> 08:37.960]  코딩이 되겠죠. 그래서 예제를 한번 보면서 실습을 해 볼게요.

[08:37.960 --> 08:45.240]  엘비트의 예제인데 이게 당뇨병 예제인데

[08:45.880 --> 08:48.120]  보시면

[08:50.320 --> 08:52.880]  명파이로

[09:02.360 --> 09:05.160]  불러드립니다.

[09:07.960 --> 09:18.960]  불러드리고 리니어 모델을 불러드립니다.

[09:19.760 --> 09:26.200]  그리고 제가 저번에 말씀드렸듯이 만약에 메플랄리브나 리프에서 실행이 안될 경우에는

[09:26.200 --> 09:28.600]  텐트

[09:28.760 --> 09:31.520]  뒷라인을 쳐서

[09:31.920 --> 09:37.240]  모듈을 다 불러드리고요.

[09:38.240 --> 09:45.200]  스펠링이 맞죠? 스펠링이 안맞아도 변수를 추정하는 것이기 때문입니다.

[09:47.840 --> 09:58.720]  그래서 이 데이터를 불러드리면 데이터는 이런식으로 나오고 타겟은 이런식으로 나오고

[09:58.720 --> 10:02.880]  히스처네임은? 콜럼네임은?

[10:03.040 --> 10:05.800]  네. 여기 나오죠?

[10:06.120 --> 10:08.600]  나옵니다.

[10:10.600 --> 10:20.360]  여기서 저희는 bmi라는 세번째 콜럼을 데이터로 활용해서

[10:20.360 --> 10:27.080]  리니어 리그레이션을 할건데 그래서 세번째 콜럼을 먼저 추출해야겠죠.

[10:29.200 --> 10:36.600]  이렇게 하면 지금 앞서 말씀드렸던 다이아 비트의 데이터를 다 뽑아낼 수 있는

[10:36.600 --> 10:40.200]  정수기이고 이거에

[10:42.160 --> 10:51.680]  세번째 콜럼을 추출합니다. 그렇죠. 이게 bmi라는 세번째 콜럼을 추출한거고

[10:51.680 --> 11:00.160]  여기서 항상 머신러닝이나 딥러닝 혹은 스태티스티컬 모델을 만들때는

[11:00.160 --> 11:05.120]  트레이닝 데이터와 테스트 데이터의 구분이 중요한데

[11:06.080 --> 11:09.520]  이것도

[11:10.200 --> 11:16.920]  직접적으로 구분을 지을 수도 있고 나중에는 제가 이것도 스케일원으로

[11:16.920 --> 11:22.320]  스플릿하는 것도 모드를 가르쳐드리도록 하겠습니다. 일단 먼저

[11:22.400 --> 11:26.880]  스플릿을 하면 트레이닝 데이터는

[11:26.880 --> 11:33.560]  잠시만요. 여기서 제가 이것도 구분을 했는데

[11:33.560 --> 11:37.520]  트레이닝 데이터는

[11:46.920 --> 11:56.920]  마이너스 20번째까지를 트레이닝 데이터로 하고

[11:57.800 --> 12:04.960]  끝에서 20번째까지를 트레이닝 데이터로 한다는 말이고

[12:09.640 --> 12:13.920]  마이너스 20번째까지를 테스트 데이터로 넣는겁니다.

[12:14.920 --> 12:21.440]  그리고 y-value에서 트레이닝은

[12:22.000 --> 12:28.640]  타깃을 이미 이 데이터 사이에서 정해놨기 때문에 타깃을

[12:35.080 --> 12:40.280]  똑같이 x-training 데이터와 똑같이 데이터 형태를, 데이터 수를

[12:40.280 --> 12:42.840]  맞춰주고

[12:46.120 --> 12:49.400]  오타가 많아가지고

[12:57.040 --> 13:07.240]  이런식으로 스플릿을 해봅니다. 그래프와 같죠? 끝에서 20번째까지의 데이터를 다 트레이닝 데이터 로

[13:07.240 --> 13:12.720]  놔두고 마지막 20번째에서 끝까지를 테스트 데이터로 놔둡니다.

[13:12.720 --> 13:20.080]  한번 쉐이프를 보면 NeoVis X가 442 x 1 데이터고

[13:20.080 --> 13:26.040]  1D Array로 타깃 데이터는 442로 됩니다.

[13:26.320 --> 13:32.280]  그래서 한번 이제 미니어 리그레이션 모델을

[13:32.280 --> 13:40.360]  생성을 해줄거에요. REGR 이라는 미니어 모델을

[13:44.720 --> 13:54.720]  생성해주고 생성한 모델에서 핏을 시키겠죠. 핏을 시킬 때는 당연히 트레이닝 데이터에 대해서

[13:54.720 --> 14:07.640]  핏을 시켜야 되고, 여기에는 테스트 데이터가 절대 들어가면 안됩니다. 테스트 데이터가 안들어가는 이런 판단 방법의 하나인 클로즈 밸리데이션도 제가 수업시간에

[14:07.640 --> 14:15.840]  한번 가르쳐드리도록 하겠습니다. 아무튼 핏을 할 때는 절대로 테스트 데이터가 들어가면 안되고

[14:16.840 --> 14:23.520]  테스트 데이터가 들어가면 안되는데, 또 역시나

[14:23.520 --> 14:26.000]  테스트 데이터가 들어가네요.

[14:26.080 --> 14:28.000]  30만 개.

[14:29.440 --> 14:31.440]  5Mhz가 들어갑니다.

[14:32.560 --> 14:34.560]  5MHz..

[14:38.200 --> 14:40.200]  50..

[14:40.760 --> 14:42.760]  51만개.

[14:44.080 --> 14:48.440]  국내 1위가 되어랈을 것 같아요.

[14:48.440 --> 14:58.440]  지금 뭔가 샘플 넘버가 안 맞는데 다시 한번 확인해 보겠습니다.

[14:58.440 --> 15:01.360]  왜 그런지

[15:08.480 --> 15:10.960]  타겟이

[15:13.720 --> 15:16.040]  잠시만요

[15:18.440 --> 15:20.760]  여기서

[15:35.840 --> 15:39.880]  어 맞는데 뭔가 지금

[15:41.580 --> 15:44.760]  다시 해볼게요

[15:44.760 --> 16:04.760]  테스트를 안하고 트레이너로 해놔서 안된것같은데

[16:04.760 --> 16:10.760]  다시 이렇게 해보면

[16:10.760 --> 16:14.760]  힛을 해보면

[16:21.760 --> 16:25.760]  그러면 힛이 완성됩니다. 모델이 완성된거고

[16:25.760 --> 16:30.760]  여기에 이제 프레딕을 해보면 결과가 나오게 될겁니다.

[16:30.760 --> 16:40.760]  결과가 나오는건 그냥 프레딕을 해서

[16:40.760 --> 16:45.760]  만약에 프레딕을 했을때

[16:45.760 --> 16:49.760]  ionfx 테스트에 대해서

[16:49.760 --> 16:54.760]  프레딕한 이 결과와 실제 y 테스트의 이 결과를 비교하면

[16:54.760 --> 16:59.760]  민스쿄어 에러를 구할수 있고 그거를 구하는게 이제

[16:59.760 --> 17:02.760]  제가 낼 과제중의 하나고

[17:02.760 --> 17:07.760]  그리고 여기서 보시면 나와있죠. mp-min에

[17:07.760 --> 17:27.760]  너무 많아서 죄송합니다.

[17:27.760 --> 17:40.760]  이런식으로 나오고

[17:40.760 --> 17:46.760]  그리고 coefficient와 intercept 아까 말씀드렸던

[17:46.760 --> 17:53.760]  기울기와 절편이죠. 절편값을 구할수도 있습니다.

[17:53.760 --> 17:56.760]  아까 말씀드렸던 공분산과

[17:56.760 --> 17:59.760]  분산으로 구할수도 있지만

[17:59.760 --> 18:09.760]  사이클론에는 정말 특별하게

[18:09.760 --> 18:14.760]  기울기랑

[18:14.760 --> 18:17.760]  이런식으로 구할수 있습니다.

[18:17.760 --> 18:20.760]  그래서 한번 그래프를 그려보면

[18:20.760 --> 18:24.760]  그렇죠. 제가 봐도 이게 민스쿄어 에러가 너무 커가지고

[18:24.760 --> 18:28.760]  보니까 그렇게 잘 된 핏은 아니네요.

[18:28.760 --> 18:32.760]  그래도 일반적으로 테스트 에러를 했을때는

[18:32.760 --> 18:35.760]  좋은 성능을 가진 모델이 될수도 있습니다.

[18:35.760 --> 18:40.760]  만약에 좀 복잡한 모델로 다항함수를 핏을 한다 하더라도

[18:40.760 --> 18:43.760]  나중에 나올 추후에 프레딕할 값이

[18:43.760 --> 18:46.760]  그냥 어떤 1차 함수의 그래프로 핏할수도 있고

[18:46.760 --> 18:51.760]  이 다항함수가 만약에 3차다항함수였을때

[18:51.760 --> 18:55.760]  나중에 테스트 데이터에 대해서는 터질수도 있겠죠.

[18:55.760 --> 19:00.760]  일단 한번 그래프를 그리고

[19:00.760 --> 19:13.760]  마무리를 할텐데

[19:13.760 --> 19:15.760]  이정은 데이터고

[19:43.760 --> 19:46.760]  이런식으로 핏한 결과를 얻을수 있습니다.