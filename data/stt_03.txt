[00:00.000 --> 00:04.240]  이제 딥러닝에 관해서 설명 드릴 건데

[00:04.240 --> 00:15.360]  이번 시간은 이론 설명 드리고 다음 시간은 파이토치 짧게 설명 드리고 파이토치로 뉴럴 네트워크  설정하는 거

[00:15.360 --> 00:21.480]  간단하게 실습해 보도록 하겠습니다. 뉴럴 네트워크는 컨볼루션 뉴럴 네트워크로 할 거고

[00:21.480 --> 00:26.160]  데이터는 패션 앱니스트로 할 예정입니다.

[00:26.560 --> 00:33.600]  그리고 강화학습이나 제가 했던 프로젝트에 대해서 한번

[00:33.600 --> 00:41.760]  실제로 어떻게 구현을 했고 어떤걸 생각했고 그리고 앞으로 좀 연구 능량이 어떤지 그런거를 하나하나

[00:41.760 --> 00:48.160]  제가 실제로 현업에 있으면서 하고 있는 일을

[00:48.160 --> 00:53.680]  보여드리는 게 좋을 것 같아서 보여드리기 전에 먼저 좀 강화학습의 개념이나

[00:53.680 --> 01:05.160]  RNN, CNN, 뉴럴 네트워크의 개념을 간단하지만 자세하게 좀 모순이 있지만 아무튼 그렇게 설명을 드리도록 하겠습니다.

[01:05.160 --> 01:12.600]  기간이 얼마 안 남아서 최대한 압축해서 제가 정리를 해서 강의를 하도록 하겠습니다.

[01:12.600 --> 01:19.320]  그리고 클러스터링 관련된 자료는 꼭 올려드리도록 할게요.

[01:19.320 --> 01:26.800]  아마 지금 제가 이 강의를 찍고 있는 이 시기에 올라왔을 수도 있겠네요.

[01:26.800 --> 01:35.600]  그리고 혹시 제가 또 해봤던 실습 중에 좀 더 좋은 실습 얘기가 있으면 보내드리도록 하겠습니다.

[01:35.600 --> 01:48.040]  수업을 하면 머신러닝은 이전에 했던 것처럼 데이터를 가지고 이제 어떤 룰이나 어떤 함수를 만드는게 머신러닝인데

[01:48.040 --> 01:57.360]  예를 들면 사람이 이제 어떤 책을 막 학습해서 있는 mc제곱 이라는 함수를 만들었듯이

[01:57.360 --> 02:03.600]  머신러닝은 어떤 데이터로부터 이게 스팸인지 햄인지 파악하는

[02:03.600 --> 02:07.640]  어떤 룰을 만드는 함수를 만드는게 머신러닝입니다.

[02:07.640 --> 02:11.760]  쉽게 생각하면 예전에 고등학교 때

[02:11.760 --> 02:18.880]  미문덕후 체험을 치는데 만약에 비분만 이렇게 공부를 했는데

[02:18.880 --> 02:25.360]  적분문제가 나오면 데이터가 없는 것을

[02:25.360 --> 02:30.000]  풀 수는 없어요. 근데 요즘은

[02:30.000 --> 02:37.560]  유사한 것을 풀 수는 있지만 미문덕후는 완전히 다른 개념이잖아요.

[02:37.560 --> 02:46.680]  그래서 이런 것을 풀 수 없는, 학습을 하지 않으면 어떤 함수를 만들 수 없는게 머신러닝의 기조로 깔려있는 개념입니다.

[02:46.680 --> 02:54.360]  그래서 오늘은 머신러닝에 포함되는 딥러닝에 대해서 설명을 드릴텐데

[02:54.360 --> 02:59.360]  딥러닝은 그렇죠 인공신경망을 알아야 뉴럴레터크를 알아야

[02:59.360 --> 03:06.400]  컨볼루션 뉴럴레터크, 리커런트 뉴럴레터크, 그리고 강학습 안에 있는 인바이러먼트에

[03:06.400 --> 03:12.080]  CNN을 꾸밀 수 있고, RNN을 꾸밀 수 있고, 뉴럴레터크 MLP로도 꾸밀 수 있는데

[03:12.080 --> 03:17.200]  그런 것을 하기 위해서 기조가 되는 인공신경망을 알아야 합니다.

[03:17.200 --> 03:23.120]  인공신경망은 이렇게 인풀레이어와 히든 레이어와 아웃풀레이어로 구성이 되어있는데

[03:23.120 --> 03:28.440]  먼저 해당하는 인풋데이터는 인풋 레이어에 위치해 있고,

[03:28.440 --> 03:33.400]  그리고 저희가 파악하고자 하는 아웃풋데이터는 아웃풋 레이어에 위치해 있습니다.

[03:33.400 --> 03:44.520]  예를 들면 제가 했던 프로젝트 중에 어떤 기상청 예측 데이터를 가지고 태양광 생산량을

[03:44.520 --> 03:50.880]  예측하는, 기상예보 데이터를 가지고 태양광 생산량을 예측하는 경우는

[03:50.880 --> 04:06.280]  기상청 예측 데이터가 풍량, 풍속, 태양포도, 강수량, 청명도 그런 식으로 있었는데

[04:06.280 --> 04:11.040]  이런게 다 인풋데이터가 되고, 인풋데이터가 인풋 레이어에 위치해 있고,

[04:11.040 --> 04:16.360]  아웃풋데이터는 태양광 생산량에 관련된게 아웃풋 레이어입니다.

[04:16.360 --> 04:22.120]  아웃풋데이터가 아웃풋 레이어에 속해 있고, 이런 식으로 구조가 되어있는데

[04:22.120 --> 04:32.960]  이러한 구조를 계산을 하기 위한, 계산을 좀 더 정확하게 하기 위한

[04:33.440 --> 04:38.120]  파라미터로서 넣는게 히든 레이어고, 그 파라미터를 업데이트 해주는게

[04:38.120 --> 04:41.400]  윗러닝 학습이라고 생각하시면 됩니다.

[04:41.400 --> 04:53.160]  인공신경만은 고등학교 때 배우셨을텐데, 이런 뉴런의 신경전달과 굉장히 유사한데요.

[04:54.720 --> 05:01.480]  이게 어떤 자극을 받으면 시냅스에서 뉴런에서 뉴런 사이로 이렇게 전달을 하는데

[05:01.480 --> 05:06.560]  이게 어떤 힘내점을 넘어야, 트레숄드를 넘어야 자극이 전달이 됩니다.

[05:06.560 --> 05:12.440]  그 트레숄드 역할을 하는게 뉴런의 애키베이션 펑션이고,

[05:12.440 --> 05:20.440]  애키베이션 펑션이 어떤 트레숄드를 넣으면 그게 전달이 되는거고,

[05:20.440 --> 05:28.800]  그 애키베이션 펑션을 업데이트 해주는게 애키베이션 펑션 안에 있는 이 웨이트를 업데이트 해주는 거고,

[05:28.800 --> 05:38.440]  웨이트가 업데이트 되면은 각각의 인풋으로부터 혹은 히든 레이어로부터 나오는 데이터의 중요도를 파악할 수 있고,

[05:38.440 --> 05:49.200]  그래서 그 중요도를 파악한 다음에, 중요도를 파악한다는게 이제 그 가중치, 그러니까 그래디언트를 계속 학습을 하면서

[05:49.200 --> 06:00.360]  적정한 그래디언트로 픽을 하고, 그게 이제 YNFX의 딥러닝의 기저가 되는겁니다.

[06:00.360 --> 06:03.040]  설명드리면

[06:04.240 --> 06:15.080]  어떤 인풋 데이터를 받아서 이 인풋 데이터가 히든 레이어로 가거나 혹은 아웃풋 레이어로 바로 갈 때

[06:15.080 --> 06:28.680]  이제 각각의 인풋 레이어, 인풋 데이터의 가중치를 설정하게 되는데, 가중치를 애키베이션 펑션이라는 함수로

[06:28.680 --> 06:37.800]  가중치의 합을 다 바꿔준 다음에 이제 그런 트레숄드를 넘었을 때나 혹은 트레숄드 안됐을 때의 값 을 또 아웃풋으로 넘겨주는 거죠.

[06:37.800 --> 06:50.880]  그래서 적정한 값을 픽하거나 아니면 적정한 값에 가까운 값으로 웨이트를 업데이트 해주는게 딥러 닝 학습이라고 생각하시면 됩니다.

[06:50.880 --> 06:59.000]  이 앱티비션 펑션은 중요한 역할을 하는데 이런 트레숄드를 정해줄 때, 트레숄드 이상의 값을 넘겨 줄 때도 중요하지만

[06:59.000 --> 07:11.760]  이런 여기서 이렇게 넘어갈 때는 y는 w1x1 플러스 b라는 1차 함수로 넘어갈 때 이런 리니어한 함수 를 액티베이션 펑션으로 넘어감으로써

[07:11.760 --> 07:19.560]  넌리니어하게 넘겨줌으로써 계산이 좀 더 복잡한 계산이 더

[07:20.000 --> 07:25.080]  적정한 값을 찾을 수 있도록 해주는 역할을 합니다.

[07:26.040 --> 07:36.760]  아까도 말씀드렸듯이 어떤 인풋 데이터가 왔을 때 가중치를 곱해주고 그리고 어떤 바이어스를 더해 줘서 넘어갈 때

[07:36.760 --> 07:47.920]  넘어가는 것은 1차 함수니까 1차 함수를 넌리니어하게 바꿔줌으로써 아웃풋에는 좀 더 복잡한

[07:47.920 --> 07:54.800]  펑션이 될 수 있도록 해주는 역할을 합니다. 왜냐하면 딥러닝을 썼다는 것 자체가

[07:54.800 --> 08:06.080]  단순한 y는 ax 플러스 b의 함수를 못 찾을 때 쓰는 거거든요. 그러니까 어떤 데이터의 불규칙성을  찾을 때는 분명히 넌리니어한 함수일 거에요.

[08:06.080 --> 08:13.360]  그래서 넌리니어한 함수를 찾기 위해서 이런 액티베이션 펑션을 쓴다고 생각하시면 됩니다.

[08:13.360 --> 08:16.920]  이쪽을 하나 보여드리면

[08:18.200 --> 08:20.480]  음

[08:21.000 --> 08:30.680]  이렇게 인풋 레이어에서 아웃풋 레이어 그리고 히든 레이어로 갈 때 이런 식으로 액티베이션 펑션을 거치면서

[08:30.680 --> 08:38.120]  어떤 식으로 인풋 데이터가 아웃풋 데이터로 가는지 나타내는 건데 이렇게 묶은 선으로 나타내거나

[08:38.120 --> 08:46.760]  그리고 가는 선으로 나타내는 거 이건 다 가중치의 크기라고 생각하시면 됩니다. 이렇게 지금 잘 도달하면은

[08:46.760 --> 08:54.640]  적정한 가중치를 찾은 거고 잘 도달하지 못하면 그건 계속 학습을 하는 과정이라고 생각하시면 됩니다.

[08:58.360 --> 09:06.240]  이렇게 계속 학습을 하면서 Objective 펑션인 끝까지 길을 찾는 쪽으로 계속 학습을 하게 됩니다.

[09:06.240 --> 09:11.720]  이 학습을 하는게 이제 가중치를 업데이트 해주는 거고

[09:11.720 --> 09:17.200]  이런식으로 찾게 되면 적정하게

[09:17.880 --> 09:21.160]  됐다고 생각하시면 됩니다.

[09:22.640 --> 09:29.160]  그래서 어떤 액티베이션 펑션을 넣었을 때는

[09:29.160 --> 09:37.320]  이제 아까도 말씀드렸던 트레저블을 넣으면 활성화 되는 거고 안되면 비활성화가 되는 건데 이 액티베이션 펑션이 하는 역할이

[09:37.320 --> 09:47.440]  런 리뉴얼하게 만들어준다고 생각하시면 됩니다. 옛날에는 스텝 펑션으로 0,2 할 때는 그냥 0으로  하거나 0,2 할 때는 1로 하는

[09:47.440 --> 09:56.360]  이런 스텝 펑션을 썼다고 하면 좀 더 런 리뉴얼하게 만들려면 여기 그림과 같이 시그모이드 함수나 아니면

[09:56.360 --> 10:05.360]  마이너스 1에서 1 사이의 값을 가지는 탄젠트 하이퍼블릭 함수를 가지거나 혹은 이제 CNN의 레이어 를 많이 겹칠 수 있는 렐루

[10:05.360 --> 10:10.240]  액티베이션 펑션을 쓴다거나 이런식으로 해서 액티베이션 펑션을 설정해 줘야 됩니다.

[10:10.240 --> 10:17.480]  이것은 액티베이션 펑션에 관해서는 제가 시즈 파일로도 드렸으니까 한번 해보셨을 거라 생각하고

[10:17.480 --> 10:20.320]  넘어가면

[10:21.320 --> 10:30.240]  넘어갑니다. 그리고 이제 딥 뉴럴 네트워크에 대해서 설명을 드릴텐데 딥 뉴럴 네트워크는 히든 레 이어가

[10:30.240 --> 10:40.360]  뉴럴 네트워크가 2개 이상의 레이어를 가지고 있는데 히든 레이어가 2개 이상 혹은 하나 이상이어도 딥 뉴럴 네트워크라고 하는데

[10:40.360 --> 10:49.400]  사실 셀로 뉴럴 네트워크라는 말을 쓰기가 조금 이제는 거의 안쓰는 용어라서 그냥 보통 히든 레이 어가 하나 있거나

[10:49.400 --> 10:54.640]  그러면 거의 다 딥 뉴럴 네트워크라고 생각하시면 됩니다.

[10:54.640 --> 11:01.560]  딥 뉴럴 네트워크의 구조가 아까 말씀드렸듯이 인풋 레이어에 인풋 데이터를 넣고 아웃풋 레이어에 아웃풋 데이터를 넣고

[11:01.560 --> 11:07.120]  그리고 히든 레이어에 히든 노드들이 있는데 이 하나하나를 노드라고 하거든요.

[11:07.120 --> 11:16.840]  이 히든 노드와 그리고 인풋 노드와 아웃풋 노드 이런식으로 구성이 되는데 이제 인풋 노드가 아웃 풋 노드로 갈 때 히든 노드를 거치고

[11:16.840 --> 11:27.320]  그 거친 값을 이제 아웃풋에서 실제 아웃풋과 비교하고 안되면 여기에 있는 가중 기질을 업데이트  해주는게 딥 뉴럴 네트워크의

[11:27.320 --> 11:32.640]  학습이라는 개념입니다. 그래서 수학적으로 좀 더 표현을 해보면

[11:32.640 --> 11:40.320]  이건 수학으로 표현하기 전에 좀 뉴럴 네트워크의 논문을 읽거나 혹시나

[11:40.320 --> 11:45.320]  fx, semicolon, seta 이런식의

[11:45.480 --> 11:52.440]  함수식을 볼 수도 있을 거에요. 아니면 보통 통계 논문이나

[11:52.440 --> 12:01.480]  보통 뉴럴 네트워크 논문에서는 이런식으로 많이 쓰이는데 이게 뭐냐면 fx를 할 때 seta를 업데이트, seta를 학습해준다 이런 개념이거든요.

[12:01.480 --> 12:07.560]  parameter, seta를 학습해서 fx를 만든다는 함수식입니다.

[12:07.880 --> 12:18.240]  그래서 forward propagation을 하면, 그러니까 propagation이 x에서 y로 갈 때 데이터의 흐름을 forward propagation이라고 하는데

[12:18.240 --> 12:27.160]  이 함수식을 그대로 풀이하면 input x에 seta1 x, 그러니까 바이어스는 뺄게요.

[12:27.160 --> 12:33.160]  seta1 x를 여기에 두고 또 여기서 이렇게 가는거, 이렇게 해서

[12:33.160 --> 12:42.320]  계속 이렇게, 이게 알파, 이게 알파라고 할께요. 알파가 activation function 이거든요.

[12:42.320 --> 12:48.840]  activation function, activation function 이런식으로 하는게 이 함수 꼬리라고 생각하시면 되고

[12:48.840 --> 12:57.160]  수학적으로 나타내면 이제 만약에 input 데이터를 1, 0, 0.5를 넣었을 때

[12:57.240 --> 13:10.520]  이제 히든레이어로 갈 때는 seta1이 지금 1.2, 1.1, 1.5, 0.3, 0.7, 0.3 입니다.

[13:10.520 --> 13:21.040]  그쵸? 이게 지금 이게 2x1행렬이고 이게 2x3행렬이잖아요.

[13:21.040 --> 13:27.600]  근데 2x3에서 2x1행렬을 곱할 수 없으니까 transpose를 시켜줍니다.

[13:27.600 --> 13:32.560]  그러면은 3x2행렬이 될거고 3x2행렬에서 2x1행렬을 곱할 수 있으니까

[13:32.560 --> 13:38.680]  여기서 이렇게 넘어갈 때는 이걸 transpose 해주고 이 값을 곱해주는 겁니다.

[13:38.680 --> 13:46.880]  그랬을 때 이제 1.35, 2.45, 1.65가 나오고 이거를 activation function 안에 넣어주면

[13:46.880 --> 13:54.240]  시그모이드 함수에 넣어주면 이런 값들이 나옵니다.

[13:54.240 --> 14:01.000]  이제 여기서 또 output으로 넘어갈 때는 지금 이 seta2가 이거 잖아요.

[14:01.480 --> 14:09.200]  그러면은 지금 이것도 2x3행렬이고

[14:09.480 --> 14:17.080]  이게 3x1행렬입니다. 그러니까 이것도 또 transpose 시켜줘서

[14:17.080 --> 14:21.200]  1x3에서 3x1을 곱할 수 있도록 만들어줍니다.

[14:21.200 --> 14:25.440]  그런 식으로 해서 최종적으로 나오는 값이 0.62고

[14:25.440 --> 14:32.640]  0.62가 실제 값과 가장 유사하도록 만드는게

[14:32.640 --> 14:40.720]  딥러닝의 목적이겠죠. 그래서 이 목적 함수는 최종 값이 실제 값과 같게

[14:40.720 --> 14:49.040]  이 오타가 미니마이지되도록 하는게 이 딥뮤럴 네트워크의 목적입니다.

[14:49.200 --> 14:56.360]  그래서 그림으로 보여드리면 이게 어떤 특정 error 그러니까

[14:56.360 --> 15:05.680]  이 forward propagation으로 한 결과와 실제 값의 오차가

[15:05.680 --> 15:12.800]  미니마이지되도록 하는게 목적이고 이 미니마이지되도록 하는 방법 중에 하나가

[15:12.800 --> 15:18.920]  이 gradient를 그러니까 이 여기서 이제 이렇게 내려가야 되잖아요.

[15:18.920 --> 15:26.320]  error가 최소화되도록 그렇게 내려가는 방식으로 업데이트 해주는게 gradient descent입니다.

[15:26.320 --> 15:32.120]  gradient를 계속 이렇게 내려가게 하는 방법

[15:32.120 --> 15:39.720]  그래서 error가 최소화되도록 하는게 gradient descent이고 이걸로 gradient descent를 통해서

[15:39.720 --> 15:46.120]  weight를 업데이트 해줍니다. weight가 아까 말했던 set하죠.

[15:46.240 --> 15:52.600]  그래서 gradient descent를 할 때 이 gradient descent를 어떻게 해주는지

[15:52.600 --> 16:01.600]  어떤 방식으로 gradient descent를 해줘야 error가 미니마이지되도록 할 수 있는지 하는 방법 중에 하나가 backpropagation입니다.

[16:01.600 --> 16:07.760]  보통 backpropagation을 보통 학습을 하는데

[16:07.760 --> 16:13.240]  이제 한번 수학적으로 보면

[16:13.240 --> 16:20.920]  이거는 지금 forward propagation이죠. 아까 말씀드렸던 방식대로 나오면 0.62가 나오는거고

[16:20.920 --> 16:28.160]  0.62에서 backpropagation을 할겁니다.

[16:28.160 --> 16:30.400]  음

[16:31.160 --> 16:37.560]  이거는 좀 뒤에 설명드리고 실제로 0.62가

[16:38.240 --> 16:45.680]  아니죠. 이거는 그냥 저희가 임의대로 정한 set1과 set2에 의해서 나온 값입니다.

[16:45.680 --> 16:54.080]  근데 실제 값은 지금 1이라고 했을 때 이제 이거의 error는 이런식으로 구할 수 있죠.

[16:54.080 --> 17:03.880]  그러면 실제 값이 1이고 0.62에 제곱해 이러면 0.072가 나오네요. 0.072가 이제 이거의 error인데

[17:03.880 --> 17:12.600]  이 error를 줄이는 방향으로. 그러니까 이 error를 계속 줄이는 방향으로 저희는 계산을 해줘야겠죠.

[17:12.600 --> 17:18.480]  근데 이 error를 지금 이렇게 나왔을 때

[17:18.480 --> 17:24.240]  지금 error term이 이렇잖아요.

[17:24.800 --> 17:33.920]  error term이 이렇게 되는데 이 error를 줄이도록 계속 순간순간 줄이도록 gradient error term인 gradient를 줄이도록 해야되는데

[17:33.920 --> 17:38.080]  이거는 당장에 지금

[17:38.320 --> 17:45.080]  여기서 구할 수가 없습니다. 왜냐하면 당장에 지금

[17:45.080 --> 17:51.480]  이 y 해색 변화율을 알 수가 없어요. 근데 그거를 체인 룰에 의해서

[17:51.480 --> 18:01.760]  체인 룰이라고 하면 만약에 dx분의 dy를 구하라고 했을 때 dx분의 dt 곱하기 dt분의 dy

[18:01.760 --> 18:08.760]  이렇게 하면 체인 룰에 의해서 만약에 이것도 예를 들면 지금 seta xk분의 seta ei

[18:08.760 --> 18:17.880]  seta yi분의 seta ei, seta sk분의 seta yi 이게 소화가 되서 똑같은 식이 되죠. 그래서 이렇게 하 나하나

[18:17.880 --> 18:21.000]  이제 하나하나의

[18:21.000 --> 18:27.160]  증분을 계산을 해주는 겁니다. 그러니까 error term에서는

[18:27.160 --> 18:33.960]  output과의 증분 그리고 output에서는 hidden layer의 두번째 hidden layer와의 증분

[18:33.960 --> 18:41.160]  그리고 첫번째 hidden layer와 두번째 hidden layer의 증분을 다 계산하기 위해서 이런 체인 룰의  개념이 들어가는 거고

[18:41.160 --> 18:48.280]  이 체인 룰의 목적은 이 error term과

[18:48.280 --> 18:55.040]  error term을 minimize 하도록 계속 gradient을 깎아주려고 이렇게 하는 겁니다.

[18:55.040 --> 19:02.000]  결국에는 이제 어떤 저희의 목적인

[19:03.000 --> 19:12.120]  이 error term을 깎기 위해서 하나하나 다 레이어별로 이렇게 할 수 있는 겁니다.

[19:12.120 --> 19:19.680]  그래서 만약에 업데이트 지금 이 seta2를 업데이트 하려면

[19:19.680 --> 19:23.760]  seta2는 지금

[19:23.760 --> 19:29.920]  이런 식으로 업데이트를 해야 되잖아요. error term에다가 seta2를

[19:29.920 --> 19:35.040]  땡값을 업데이트 해줘야지 그 seta2에 해당 seta2가

[19:35.040 --> 19:42.600]  error term이 minimized 되도록 업데이트가 되는 건데 이제 이 dseta-de가

[19:42.600 --> 19:50.320]  이런 식으로 나옵니다. 이런 식이라는게 지금 이 yi-de가 이거 잖아요.

[19:50.320 --> 19:58.320]  이거에서 아까도 보여드렸듯이 체인 룰을 쓰면, 아 이게 칠판이 없으니까 제가 지금 좀 헷갈리네요.

[19:58.320 --> 20:10.040]  아무튼 체인 룰을 쓰시면 이게 dseta2-dyi 그리고 dyi-dei

[20:10.040 --> 20:15.320]  이런 식으로 seta2-dei가 계산이 되는 거고

[20:15.320 --> 20:20.080]  이게 마이너스 0.38이었고 이거는 구할 수 있고

[20:20.080 --> 20:25.960]  이거는 왜 구할 수 있는지 알겠죠? 이게 sigmoid 하면서 그냥 미분하면 되는 거니까

[20:25.960 --> 20:31.040]  이런 식으로 해서 구하면 seta2가

[20:31.040 --> 20:35.240]  어 이게 이런 식으로 나오고

[20:35.240 --> 20:43.160]  이제 기존의 값에서 이 증분을 어떤 rate를 뽑은 값을 빼주면

[20:43.160 --> 20:50.120]  업데이트 된 seta2가 됩니다. 이게 빼준다는게 어떤 가중치를 업데이트 하려면

[20:50.120 --> 20:56.920]  제가 이 식을 따왔는데 이게 제일 이해가 쉽더라구요.

[20:56.920 --> 21:04.840]  l을 낮추는 방향은 gradient descent에서 마이너스고 어떤 발자국 크기라고 해야 되나요?

[21:04.840 --> 21:11.400]  그래디언트를 깎는 크기 그러니까 만약에 이게 너무 크면

[21:11.400 --> 21:18.320]  어 이게 지금 작으면 이런 식으로 조금씩 올릴텐데 이게 너무 크면

[21:18.360 --> 21:23.040]  이런 식으로 이런 식으로 이런 식으로 해서 수렴도 안되고 이렇게 발산을 할 수도 있습니다.

[21:23.040 --> 21:34.440]  그쵸? 그래서 이 한 발자국의 크기가 중요한 거고 현지점의 기울기가 이제 그 그래디언트에 해당하 는 겁니다.

[21:35.680 --> 21:44.720]  그래서 seta2를 gamma1에 대해서 업데이트를 하는 거고 seta1도 유사하게 하면 되는 거죠.

[21:44.760 --> 21:50.480]  그래서 그래디언트 센터를 할 때 이 gamma를 정하는게 보통 1으로 안정하거든요.

[21:50.480 --> 21:57.280]  0.01이나 0.001이나 아주 작은 값으로 정하고 왜냐하면 아까도 말씀드렸듯이 너무 큰 값으로 하면

[21:57.280 --> 22:06.640]  이렇게 그래디언트를 깎아내릴 때 발산을 해버리기 때문에 gamma는 좀 작은 값으로 하는게 맞구요.

[22:06.640 --> 22:18.240]  그리고 근데 너무 작은 값으로 하면은 만약에 지금 2차항수지만 보통 에러가 이렇게 막 여러가지로 이렇게

[22:18.240 --> 22:23.360]  이게 몇차항이죠? 아무튼 n차항으로 되어있을 때는

[22:23.360 --> 22:29.880]  local minimum 이라고 만약에 이런 4차항일 때는 여기에 빠져서 못 나올 수도 있거든요.

[22:29.920 --> 22:34.720]  그래서 gamma값을 정하는 것도 굉장히 중요합니다.

[22:34.720 --> 22:41.720]  그래서 seta1과 seta2를 이런식으로 업데이트 하는 거. 업데이트하는 방식 중의 하나인 backpropagation 입니다.

[22:41.720 --> 22:48.680]  backpropagation은 거의 보통 잘 찾으니까 잘 기억해 두시면 좋습니다.

[22:49.960 --> 22:54.240]  수식어로 나타나 있는거 이거는 제가 꼭 올려드리도록 할게요.

[22:54.240 --> 23:01.840]  이게 칠판에 제가 쓰면서 하면은 잘 되는데 이게 컴퓨터로 지금 제가 하나하나 설명을 하려고 하니 까

[23:01.840 --> 23:10.440]  조금 헷갈리네요. 이게 어쨌든 이게 지금 체인 룰 이라는 공식에 있어서 이제 하나하나에

[23:10.440 --> 23:17.720]  seta k, seta k-1 이런 weight를 업데이트 해주는걸

[23:17.720 --> 23:26.200]  backpropagation의 목적입니다. 이 backpropagation의 objective function은 실제로

[23:26.200 --> 23:32.720]  seta k-1, seta k-2, seta k를 initial한 값을 정한 다음에

[23:32.720 --> 23:40.520]  forward propagation 하고 그 값에 실제 값과 그 값의 error term을 구한 다음에 그 error가 minimized 하도록 하는게

[23:40.520 --> 23:48.920]  objective function이고, 이 objective function을 그때그때 이제 미분해서 빼주는게

[23:48.920 --> 23:57.560]  gradient descent이고, gradient descent를 빼줄 때 이제 이 하나하나의 error term을 구해주는게 backpropagation입니다.

[23:57.560 --> 24:06.120]  이렇게 해두시면 되겠습니다. 그래서 gradient descent는 설명 드렸는데

[24:06.720 --> 24:09.880]  gradient descent를 할 때

[24:12.280 --> 24:17.800]  이런 running rate도 중요하고 loss function도 중요한데 아까도 말씀드렸듯이

[24:17.800 --> 24:25.200]  local minimum에 빠질 수가 있어요. 그리고 또 발산을 할 수도 있고. 이렇게 적절하게만 가면 정말 좋은데

[24:25.200 --> 24:34.280]  정말 적절하게 가기가 힘들거든요. 그래서 여러분들이 텐서플로나 파이토치나 테라스를 한번 써보셨을거에요.

[24:34.280 --> 24:40.520]  만약에 써봤을때 sgd 라고 나오는데 stochastic gradient descent 라고 하는데

[24:40.520 --> 24:45.920]  이거는 이제 예를 들면

[24:45.920 --> 24:53.880]  어떤 값에 있어서 이렇게 랜덤하게 initialize 한 다음에 이렇게

[24:53.880 --> 24:56.840]  만약에 아까처럼

[24:57.680 --> 25:05.880]  gradient descent를 할 때 전체 데이터에 대해서 전부 다 gradient descent를 해서 시간이 많이 걸 리고 혹은 아까도 말씀드렸던 local minimum에 빠질 수 있는데

[25:05.880 --> 25:14.120]  stochastic gradient descent라면 일부 데이터를 loss function으로 하고

[25:14.120 --> 25:20.680]  이거를 gradient descent을 하기 때문에 좀 더 local minimum에 빠지는 확률을 줄여둘 수도 있고

[25:20.680 --> 25:27.800]  좀 더 계산시간도 좀 더 빨리 잡을 수 있습니다. 근데 이게 또

[25:27.800 --> 25:34.160]  꼭 stochastic gradient descent를 한다고 해서 gradient descent보다 더

[25:34.160 --> 25:41.320]  물론 잘 나올 확률도 크지만 그렇게 꼭 정확하게 잘 나오진 않거든요. 왜냐하면 실제로 글로벌 미니멈에 가야 되는데

[25:41.320 --> 25:48.680]  local minimum에 빠지는 경우도 굉장히 많습니다. 뭐 이런식으로 함수가 복잡할 때 정말 local minimum이 많겠죠.

[25:48.680 --> 25:55.360]  정말 필수점이 있는데 글로벌하게 저희가 생각하는 minimized한 값이 있지만 이렇게

[25:55.360 --> 26:03.200]  변곡점이 많을 때는 local minimum에 빠질 수 있는데 이거를 해결하기 위한 방법이 momentum

[26:03.200 --> 26:15.120]  gradient descent입니다. 이게 다 모듈에도 있거든요. gradient descent나 stochastic gradient descent을 할 때 momentum을 선정하는게 보통 다 있습니다.

[26:15.120 --> 26:21.640]  momentum 이거는 좀 복잡하지만 그냥 설명하면 그냥 최상의 방법으로

[26:21.640 --> 26:27.600]  지금 sgd 같은 경우에 이렇게 fluctuation이 많이 생기는 경우가 있어서

[26:27.600 --> 26:33.400]  글로벌 미니멈으로 잘 안가는 경우가 많거든요. 많지는 않은데 발생할 수 있거든요.

[26:33.400 --> 26:40.920]  그때를 위해서 글로벌 미니멈으로 갈 수 있도록 가속도를 주는 거죠. momentum 이라고. momentum은 다 아시죠?

[26:40.920 --> 26:48.520]  물리시간의 반성의 힘으로 이렇게 가는 것을 momentum 이라고 하는데

[26:48.520 --> 26:58.360]  오틀레이션도 줄이고 종국적으로 글로벌 미니멀과 로벌 미니멀은 사실 정의하기는 힘듭니다.

[26:58.360 --> 27:07.400]  아무튼 minimized로 하기 위해서 가장 최선의 방법으로 가는게 momentum gradient descent입니다.

[27:07.400 --> 27:16.160]  그래서 지금 나오는 모듈에 보통 gradient descent나 stochastic gradient descent에 momentum을 설정할 수 있게 해놨을 거에요.

[27:16.160 --> 27:23.800]  그거를 적절하게 설정하면 gradient descent나 stochastic gradient descent를 잘 활용할 수 있습니다.

[27:23.800 --> 27:31.160]  그러면 좀 더 빠르고 정확하게 자중시 업데이트를 할 수 있겠죠.

[27:31.160 --> 27:45.120]  실습 시간인데 실습은 제가 여기서 강의를 끊고 실습 강의를 또 바로 다음주에 하나 더 올리도록 하겠습니다.

[27:45.120 --> 27:47.800]  수고하셨습니다.

